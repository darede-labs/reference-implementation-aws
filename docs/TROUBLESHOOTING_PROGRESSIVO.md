# üêõ Troubleshooting Progressivo: Problemas Reais Encontrados

> **Prop√≥sito**: Documentar CADA erro/problema encontrado durante implementa√ß√£o com solu√ß√£o EXATA aplicada
> **N√ÉO √©**: Lista gen√©rica de "poss√≠veis problemas"
> **√â**: Registro cronol√≥gico de problemas REAIS que aconteceram

---

## üìã FORMATO DE CADA ENTRADA

```markdown
## PROBLEMA #XXX: T√≠tulo do Problema

**Data encontrado:** YYYY-MM-DD HH:MM
**Fase:** [SETUP / CONFIG / TERRAFORM / INSTALL / DEPLOY / CLEANUP]
**Severidade:** üî¥ Bloqueante / üü° Warning / üü¢ Leve

### Contexto
O que voc√™ estava tentando fazer quando o erro aconteceu.

### Sintoma / Erro
```
Mensagem de erro EXATA (copiar e colar)
```

### Comando que causou
```bash
comando exato que gerou o erro
```

### Causa Raiz
An√°lise t√©cnica do QUE causou o problema (n√£o como resolver ainda).

### Solu√ß√£o Aplicada
Passos EXATOS executados para resolver:

1. Primeiro comando
2. Segundo comando
3. Etc

### Valida√ß√£o
Como confirmar que foi resolvido:
```bash
comando de valida√ß√£o
# output esperado
```

### Preven√ß√£o Futura
Como evitar que aconte√ßa novamente:
- Ajuste em config
- Valida√ß√£o pr√©-emptiva
- Documenta√ß√£o atualizada

### Tempo Perdido
X minutos

### Refer√™ncias
- [Link 1]
- [Link 2]

---
```

---

## üö® PROBLEMAS DOCUMENTADOS

### üìä ESTAT√çSTICAS

**Total de problemas:** 0 (atualizar conforme encontrar)
**Bloqueantes resolvidos:** 0
**Warnings ignorados:** 0
**Tempo total perdido:** 0 minutos

**Por categoria:**
- Setup: 0
- Config: 0
- Terraform: 0
- Install: 0
- Deploy: 0
- Cleanup: 0

---

## [EXEMPLO] PROBLEMA #001: VPC Limit Exceeded

**Data encontrado:** 2024-12-09 14:32
**Fase:** TERRAFORM
**Severidade:** üî¥ Bloqueante

### Contexto

Estava executando `terraform apply` para criar o cluster EKS. Regi√£o us-east-1 j√° tinha VPCs de outros projetos.

### Sintoma / Erro

```
Error: Error creating VPC: VpcLimitExceeded: The maximum number of VPCs has been reached.
	status code: 400, request id: a1b2c3d4-e5f6-7890-abcd-ef1234567890

  on .terraform/modules/vpc/main.tf line 15, in resource "aws_vpc" "this":
  15: resource "aws_vpc" "this" {
```

### Comando que causou

```bash
terraform apply tfplan
```

### Causa Raiz

Conta AWS tem limite padr√£o de **5 VPCs por regi√£o**. A conta j√° tinha 5 VPCs de projetos antigos, impossibilitando criar mais.

**Verifica√ß√£o do limite:**
```bash
aws ec2 describe-vpcs --region us-east-1 --query 'Vpcs[*].VpcId'
```

**Output:**
```json
[
  "vpc-0a1b2c3d4e5f67890",  // projeto-antigo-1
  "vpc-1a2b3c4d5e6f78901",  // projeto-antigo-2
  "vpc-2a3b4c5d6e7f89012",  // default
  "vpc-3a4b5c6d7e8f90123",  // teste-abandonado
  "vpc-4a5b6c7d8e9f01234"   // lab-dev
]
```

Total: 5/5 VPCs (limite atingido)

### Solu√ß√£o Aplicada

**Op√ß√£o 1: Deletar VPCs n√£o utilizadas (escolhida)**

```bash
# 1. Identificar VPC n√£o utilizada
aws ec2 describe-instances --region us-east-1 \
  --filters "Name=vpc-id,Values=vpc-3a4b5c6d7e8f90123" \
  --query 'Reservations[].Instances[].InstanceId'
# Output: [] (sem inst√¢ncias)

# 2. Deletar recursos dependentes primeiro
# 2.1 Internet Gateway
aws ec2 detach-internet-gateway --region us-east-1 \
  --internet-gateway-id igw-0abc123 \
  --vpc-id vpc-3a4b5c6d7e8f90123

aws ec2 delete-internet-gateway --region us-east-1 \
  --internet-gateway-id igw-0abc123

# 2.2 Subnets
aws ec2 delete-subnet --region us-east-1 --subnet-id subnet-abc123
aws ec2 delete-subnet --region us-east-1 --subnet-id subnet-def456

# 2.3 Security Groups (exceto default)
aws ec2 delete-security-group --region us-east-1 --group-id sg-abc123

# 3. Deletar VPC
aws ec2 delete-vpc --region us-east-1 --vpc-id vpc-3a4b5c6d7e8f90123
```

**Output:**
```
(sem output = sucesso)
```

**Op√ß√£o 2: Aumentar limite via Support (n√£o usado)**
- Abrir ticket no AWS Support
- Solicitar aumento para 10 VPCs
- Esperar 1-2 dias √∫teis

### Valida√ß√£o

```bash
# Verificar VPCs agora
aws ec2 describe-vpcs --region us-east-1 --query 'Vpcs[*].VpcId' | jq 'length'
# Output: 4

# Tentar terraform apply novamente
terraform apply
```

**Resultado:** ‚úÖ Sucesso - VPC criada com sucesso

### Preven√ß√£o Futura

1. **Antes de cada deploy:**
   ```bash
   # Verificar limites
   aws ec2 describe-account-attributes --region us-east-1 \
     --attribute-names vpc-max-number

   # Contar VPCs existentes
   aws ec2 describe-vpcs --region us-east-1 --query 'Vpcs | length'
   ```

2. **Documentar no pr√©-requisitos:**
   - Adicionar checklist: "Verificar disponibilidade de VPC quota"
   - Script de valida√ß√£o pr√©-apply

3. **Limpeza peri√≥dica:**
   - Agendar revis√£o trimestral de VPCs n√£o utilizadas
   - Tag todas VPCs com data de cria√ß√£o e projeto

### Tempo Perdido

**15 minutos** (diagn√≥stico 5 min + cleanup 8 min + re-apply 2 min)

### Refer√™ncias

- [AWS VPC Limits](https://docs.aws.amazon.com/vpc/latest/userguide/amazon-vpc-limits.html)
- [How to delete VPC](https://docs.aws.amazon.com/vpc/latest/userguide/delete-vpc.html)
- [Request limit increase](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html)

---

## [EXEMPLO] PROBLEMA #002: Spot Instance Insufficient Capacity

**Data encontrado:** 2024-12-09 14:47
**Fase:** TERRAFORM
**Severidade:** üü° Warning (resolveu automaticamente)

### Contexto

Terraform estava criando node group Spot. Regi√£o us-east-1a n√£o tinha capacidade t3.medium Spot dispon√≠vel temporariamente.

### Sintoma / Erro

**N√£o foi erro fatal**, apenas observado no CloudWatch Events:

```json
{
  "version": "0",
  "id": "abc123-def456",
  "detail-type": "EC2 Spot Instance Interruption Warning",
  "source": "aws.ec2",
  "account": "123456789012",
  "time": "2024-12-09T14:47:32Z",
  "region": "us-east-1",
  "resources": [],
  "detail": {
    "instance-id": "",
    "instance-action": "terminate",
    "reason": "InsufficientInstanceCapacity"
  }
}
```

### Comando que causou

```bash
terraform apply tfplan
```

Durante cria√ß√£o do `aws_eks_node_group.this["spot_nodes"]`

### Causa Raiz

**Spot capacity √© din√¢mica.** Temporariamente, AZ us-east-1a n√£o tinha t3.medium Spot dispon√≠vel.

**Por que n√£o falhou:**
- Node group configurado com **m√∫ltiplos instance types**: `["t3.medium", "t3a.medium", "t2.medium"]`
- Node group configurado para **m√∫ltiplas AZs**: `[us-east-1a, us-east-1b, us-east-1c]`
- AWS automaticamente tentou pr√≥xima combina√ß√£o

### Solu√ß√£o Aplicada

**Nenhuma a√ß√£o necess√°ria.** Terraform retentou automaticamente:

```
module.eks.aws_eks_node_group.this["spot_nodes"]: Still creating... [2m10s elapsed]
module.eks.aws_eks_node_group.this["spot_nodes"]: Still creating... [2m20s elapsed]
```

N√≥s foram criados em us-east-1b e us-east-1c ao inv√©s de us-east-1a.

### Valida√ß√£o

```bash
kubectl get nodes -o wide
```

**Output:**
```
NAME                             STATUS   INTERNAL-IP    EXTERNAL-IP   AZ
ip-10-0-67-92.ec2.internal       Ready    10.0.67.92     3.x.x.x       us-east-1b
ip-10-0-98-143.ec2.internal      Ready    10.0.98.143    3.x.x.x       us-east-1c
```

‚úÖ N√≥s criados em AZs alternativas, tudo funcionando.

### Preven√ß√£o Futura

**J√° est√° implementado:**
- ‚úÖ M√∫ltiplos instance types no array
- ‚úÖ Distribui√ß√£o em 3 AZs
- ‚úÖ AWS Node Termination Handler (para prod, monitorar interrup√ß√µes)

**Para produ√ß√£o, adicionar:**

```hcl
# Aumentar diversidade de instance types
instance_types = [
  "t3.medium",
  "t3a.medium",
  "t2.medium",
  "t3.small",   # Fallback menor
  "m5.large"    # Fallback diferente family
]
```

### Tempo Perdido

**0 minutos** (resolveu automaticamente durante terraform apply)

### Refer√™ncias

- [Spot Best Practices](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-best-practices.html)
- [EKS Node Group Spot](https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html#managed-node-group-capacity-types)
- [Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/)

---

## [EXEMPLO] PROBLEMA #003: Let's Encrypt Rate Limit

**Data encontrado:** 2024-12-09 15:22
**Fase:** INSTALL
**Severidade:** üü° Warning

### Contexto

Cert-manager estava tentando emitir certificados SSL para todos ingresses. Dom√≠nio `timedevops.click` j√° havia solicitado 4 certificados no dia (testes anteriores).

### Sintoma / Erro

```bash
kubectl describe certificate -n backstage backstage-tls
```

**Output:**
```
Events:
  Type     Reason        Age   From          Message
  ----     ------        ----  ----          -------
  Warning  FailedCreate  2m    cert-manager  Failed to create Order: 429 Too Many Requests: Error creating new order :: too many certificates already issued for exact set of domains: timedevops.click: see https://letsencrypt.org/docs/rate-limits/
```

### Comando que causou

```bash
./scripts/install.sh
```

Especificamente durante cria√ß√£o de Certificate resources pelo cert-manager.

### Causa Raiz

**Let's Encrypt Production tem rate limits:**
- **50 certificados/dom√≠nio/semana**
- **5 certificados/exato conjunto de dom√≠nios/semana**

Dom√≠nio `timedevops.click` j√° tinha solicitado:
1. backstage.timedevops.click (teste 1)
2. argocd.timedevops.click (teste 1)
3. backstage.timedevops.click (teste 2 - refazer)
4. argocd.timedevops.click (teste 2 - refazer)
5. **backstage.timedevops.click (ATUAL - NEGADO)** ‚Üê Limite atingido

### Solu√ß√£o Aplicada

**Op√ß√£o 1: Aguardar 1 semana (n√£o escolhida)**

**Op√ß√£o 2: Usar staging Let's Encrypt temporariamente (escolhida)**

```bash
# 1. Editar ClusterIssuer para usar staging
kubectl edit clusterissuer letsencrypt-prod -n cert-manager
```

**Alterar:**
```yaml
# DE:
server: https://acme-v02.api.letsencrypt.org/directory

# PARA:
server: https://acme-staging-v02.api.letsencrypt.org/directory
```

```bash
# 2. Deletar certificates existentes para for√ßar reemiss√£o
kubectl delete certificate --all -n backstage
kubectl delete certificate --all -n argocd

# 3. Aguardar recreate autom√°tico (cert-manager detecta)
kubectl get certificate -A --watch
```

**Resultado ap√≥s 3 minutos:**
```
NAMESPACE   NAME            READY   AGE
backstage   backstage-tls   True    2m
argocd      argocd-tls      True    2m
```

**‚ö†Ô∏è Aviso:** Certificados staging geram warning no browser (n√£o confi√°vel), mas funcionam para testes.

**Op√ß√£o 3: Usar subdom√≠nios diferentes (alternativa)**
```yaml
# Trocar backstage.timedevops.click ‚Üí backstage-v2.timedevops.click
# Evita rate limit do "exact set of domains"
```

### Valida√ß√£o

```bash
# Verificar certificado emitido
kubectl get certificate backstage-tls -n backstage -o yaml | grep issuer

# Output:
# issuer: (STAGING) Artificial Apricot R3
```

```bash
# Testar acesso (vai dar warning de certificado)
curl -k https://backstage.timedevops.click
```

**Browser:** Mostra "Not Secure" mas p√°gina carrega ‚úÖ

### Preven√ß√£o Futura

1. **Usar staging durante desenvolvimento:**
   - Padr√£o em ClusterIssuer deve ser staging
   - Mudar para production apenas em deploy final

2. **Documentar no guia:**
   ```markdown
   ‚ö†Ô∏è IMPORTANTE: Let's Encrypt produ√ß√£o tem limite de 5 certs/semana
   Para testes, use staging:
   clusterIssuer: letsencrypt-staging
   ```

3. **Monitorar rate limits:**
   ```bash
   # Verificar quantos certs foram solicitados
   curl "https://crt.sh/?q=%.timedevops.click&output=json" | jq 'length'
   ```

4. **Para produ√ß√£o:**
   - Planejar certificado wildcard: `*.timedevops.click`
   - Requer valida√ß√£o DNS (mais complexo mas evita rate limit)

### Tempo Perdido

**8 minutos** (diagn√≥stico 3 min + trocar para staging 2 min + aguardar 3 min)

### Refer√™ncias

- [Let's Encrypt Rate Limits](https://letsencrypt.org/docs/rate-limits/)
- [Cert-Manager Staging](https://cert-manager.io/docs/configuration/acme/#creating-a-basic-acme-issuer)
- [Check certificate log](https://crt.sh/)

---

## PROBLEMA #004: Sem permiss√£o para criar GitHub App

**Data encontrado:** 2024-12-09 13:30
**Fase:** GITHUB
**Severidade:** üî¥ Bloqueante

### Contexto

Tentando criar GitHub App usando Backstage CLI ou interface web do GitHub na organiza√ß√£o `darede-labs`.

### Sintoma / Erro

**Na interface GitHub:**
```
Error: You need to be an organization owner to create GitHub Apps
```

**Via Backstage CLI:**
```bash
npx @backstage/cli@latest create-github-app darede-labs

# Error: Forbidden - requires organization owner permissions
```

### Comando que causou

```bash
npx @backstage/cli@latest create-github-app darede-labs
```

### Causa Raiz

**GitHub requer permiss√£o de Owner para criar Apps.** Membros com role "Member" n√£o podem criar GitHub Apps, mesmo com permiss√µes de admin em reposit√≥rios.

**Verificar sua role:**
```bash
# Via CLI
gh api /orgs/darede-labs/members/[SEU-USERNAME] | jq .role

# Via web
# https://github.com/orgs/darede-labs/people
# Procurar seu nome e ver a role
```

Roles do GitHub:
- ‚úÖ **Owner**: Pode criar Apps
- ‚ùå **Member**: N√£o pode criar Apps
- ‚ùå **Billing manager**: N√£o pode criar Apps

### Solu√ß√£o Aplicada

**Op√ß√£o 1: Solicitar upgrade para Owner (escolhida)**

1. Contactar Owner atual da organiza√ß√£o
2. Solicitar upgrade tempor√°rio para Owner
3. Criar os GitHub Apps
4. (Opcional) Retornar para Member ap√≥s criar

**Op√ß√£o 2: Owner criar os Apps e compartilhar credenciais**

1. Owner acessa: `https://github.com/organizations/darede-labs/settings/apps/new`
2. Cria o App seguindo guia
3. Gera private key
4. Compartilha arquivo de credenciais de forma segura (1Password, etc)

**Op√ß√£o 3: Usar GitHub App global (n√£o recomendado para prod)**

Criar App na conta pessoal ao inv√©s da org (menos seguro).

### Valida√ß√£o

```bash
# Verificar que voc√™ √© Owner
gh api /orgs/darede-labs/memberships/[USERNAME]

# Output esperado:
# "role": "admin"  # ou "owner" dependendo da API
```

Depois criar o App novamente:
```bash
npx @backstage/cli@latest create-github-app darede-labs
# ‚úÖ Deve funcionar agora
```

### Preven√ß√£o Futura

1. **Documentar no README**:
   ```markdown
   ### Pr√©-requisitos GitHub
   - ‚ö†Ô∏è Permiss√£o de **Owner** na organiza√ß√£o
   - Verificar antes de iniciar implementa√ß√£o
   ```

2. **Checklist pr√©-implementa√ß√£o**:
   - [ ] Verificar permiss√µes GitHub
   - [ ] Verificar permiss√µes AWS
   - [ ] Verificar ferramentas instaladas

3. **Para implementa√ß√µes em cliente**:
   - Solicitar permiss√£o Owner antes de agendar
   - Ou pedir que Owner participe da cria√ß√£o dos Apps
   - Documentar quem tem permiss√£o Owner

### Tempo Perdido

**15 minutos** (descoberta 5 min + solicitar permiss√£o 10 min)

### Refer√™ncias

- [GitHub Org Roles](https://docs.github.com/en/organizations/managing-peoples-access-to-your-organization-with-roles/roles-in-an-organization)
- [Creating GitHub Apps](https://docs.github.com/en/apps/creating-github-apps/registering-a-github-app/registering-a-github-app)
- [GitHub App Permissions](https://docs.github.com/en/apps/creating-github-apps/creating-github-apps/choosing-permissions-for-a-github-app)

---

## PROBLEMA #005: Service Control Policy bloqueando Secrets Manager

**Data encontrado:** 2024-12-09 14:26
**Fase:** INSTALL
**Severidade:** üî¥ CR√çTICA - Bloqueio total da instala√ß√£o

### Contexto
Ap√≥s criar o cluster EKS com sucesso (86 recursos) e instalar ArgoCD e External Secrets Operator, a sincroniza√ß√£o de secrets do AWS Secrets Manager para o Kubernetes falhou. Os ExternalSecrets ficaram em estado `SecretSyncedError`.

### Sintoma / Erro
```bash
kubectl describe externalsecret hub-cluster-secret -n argocd

Events:
  Warning  UpdateFailed  AccessDeniedException:
  User: arn:aws:sts::948881762705:assumed-role/external-secrets-20251209164024175400000002/eks-idp-poc-cl-external-s-XXX
  is not authorized to perform: secretsmanager:GetSecretValue
  on resource: cnoe-ref-impl/config
  with an explicit deny in a service control policy
  status code: 400
```

### Comando que causou
```bash
# Script de instala√ß√£o que aplica ExternalSecrets
echo "yes" | ./scripts/install.sh
```

### Causa Raiz
A conta AWS `948881762705` est√° em uma **AWS Organization** com **Service Control Policies (SCPs)** que negam explicitamente acesso ao AWS Secrets Manager.

**Hierarquia AWS:**
```
SCP (Organization) ‚Üê DENY expl√≠cito aqui
  ‚Üì
IAM Policy (Role) ‚Üê ALLOW (mas n√£o funciona)
  ‚Üì
Pod Identity ‚Üê Bloqueado
```

**SCPs t√™m preced√™ncia absoluta sobre IAM policies.** Mesmo a role tendo permiss√µes corretas, a SCP da organiza√ß√£o sobrescreve e nega.

### Solu√ß√£o Aplicada
**Decis√£o:** Destruir infraestrutura e aguardar libera√ß√£o da SCP

```bash
cd cluster/terraform
terraform destroy -auto-approve
```

**Motivo:** Evitar custos desnecess√°rios (~$0.17/hora) enquanto aguarda resolu√ß√£o organizacional.

### Investiga√ß√£o Realizada

1. ‚úÖ **Verificar se secret existe:**
   ```bash
   aws secretsmanager describe-secret --secret-id "cnoe-ref-impl/config"
   # ‚úÖ Secret existe e ARN correto
   ```

2. ‚ùå **Problema encontrado na IAM policy:** Regi√£o errada
   ```json
   // ANTES: us-west-2 (errado)
   "Resource": ["arn:aws:secretsmanager:us-west-2:948881762705:secret:cnoe-ref-impl/*"]

   // DEPOIS: wildcard (corrigido)
   "Resource": ["arn:aws:secretsmanager:*:948881762705:secret:cnoe-ref-impl/*"]
   ```

3. ‚úÖ **Atualizar pol√≠tica via Terraform:**
   ```bash
   terraform apply -target=module.external_secrets_pod_identity
   ```

4. ‚úÖ **Reiniciar pods:**
   ```bash
   kubectl rollout restart deployment -n external-secrets
   ```

5. ‚ùå **Erro persiste:** Confirmado que √© SCP, n√£o IAM policy

### Alternativas Consideradas (n√£o implementadas)

**Op√ß√£o 1:** Ajustar SCP (requer Admin Organization)
```bash
# Adicionar exce√ß√£o na SCP para:
"Resource": "arn:aws:secretsmanager:*:*:secret:cnoe-ref-impl/*"
```

**Op√ß√£o 2:** Bypass com Kubernetes Secrets nativos
```bash
kubectl create secret generic hub-cluster -n argocd \
  --from-literal=name=idp-poc-cluster \
  --from-literal=server=https://kubernetes.default.svc
kubectl label secret hub-cluster -n argocd \
  argocd.argoproj.io/secret-type=cluster \
  environment=control-plane \
  path_routing=false
```

**Op√ß√£o 3:** Usar SSM Parameter Store ao inv√©s de Secrets Manager

### Valida√ß√£o
- [x] Secret existe no Secrets Manager
- [x] IAM policy corrigida (regi√£o wildcard)
- [x] Erro explicitamente menciona "SCP"
- [x] External Secrets Operator rodando (3/3 pods)
- [x] ArgoCD instalado (6/6 pods)
- [x] N√£o √© problema de permiss√£o IAM

### Preven√ß√£o Futura

1. **Checklist pr√©-implementa√ß√£o:**
   - [ ] Verificar SCPs da AWS Organization
   - [ ] Testar acesso aos servi√ßos necess√°rios via assume role
   - [ ] Confirmar que Secrets Manager n√£o est√° bloqueado

2. **No guia de instala√ß√£o:**
   ```markdown
   ### ‚ö†Ô∏è Requisito: Service Control Policies

   Se sua conta est√° em AWS Organization, verifique que as SCPs permitem:
   - `secretsmanager:GetSecretValue`
   - `secretsmanager:DescribeSecret`
   - Na regi√£o onde o cluster ser√° criado

   Comando para testar:
   aws secretsmanager list-secrets --region us-east-1
   ```

3. **Teste de Pod Identity antes de provisionar:**
   ```bash
   # Criar role tempor√°ria e testar assume
   aws sts assume-role --role-arn arn:aws:iam::ACCOUNT:role/test-pod-identity
   aws secretsmanager get-secret-value --secret-id cnoe-ref-impl/config
   ```

### Pr√≥ximos Passos

1. **Solicitar ao Admin da Organization:**
   - Ajustar SCP para permitir Secrets Manager
   - Ou: Exce√ß√£o para secrets `cnoe-ref-impl/*`
   - Regi√£o: us-east-1

2. **Ap√≥s libera√ß√£o:**
   ```bash
   # Reprovisionar cluster
   terraform apply

   # Validar acesso
   kubectl describe externalsecret hub-cluster-secret -n argocd
   # Deve mostrar: Status: SecretSynced
   ```

### Tempo Perdido
- Instala√ß√£o ArgoCD/External Secrets: 10 min
- Investiga√ß√£o erro: 10 min
- Corre√ß√£o IAM policy + testes: 10 min
- **Total: 30 minutos**

### Custo Gerado
- Cluster rodou ~45 min antes do destroy
- EKS Control Plane: $0.10/h √ó 0.75h = $0.075
- 2√ó Spot t3.medium: $0.025/h √ó 0.75h = $0.019
- NAT Gateway: $0.045/h √ó 0.75h = $0.034
- **Total aproximado: $0.13**

### Refer√™ncias
- [AWS SCPs](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html)
- [IAM Policy Evaluation](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html)
- [External Secrets AWS Provider](https://external-secrets.io/latest/provider/aws-secrets-manager/)

---

## üìù TEMPLATE PARA NOVOS PROBLEMAS

### PROBLEMA #XXX: T√≠tulo Descritivo

**Data encontrado:** YYYY-MM-DD HH:MM
**Fase:** [SETUP / CONFIG / TERRAFORM / INSTALL / DEPLOY / CLEANUP]
**Severidade:** üî¥ / üü° / üü¢

### Contexto
[O que voc√™ estava fazendo]

### Sintoma / Erro
```
[Copiar erro EXATO]
```

### Comando que causou
```bash
comando
```

### Causa Raiz
[An√°lise t√©cnica]

### Solu√ß√£o Aplicada
1. Passo 1
2. Passo 2

### Valida√ß√£o
```bash
comando de valida√ß√£o
```

### Preven√ß√£o Futura
- Item 1
- Item 2

### Tempo Perdido
X minutos

### Refer√™ncias
- [Link]

---

## üéØ DIRETRIZES DE USO

### Quando adicionar aqui?

‚úÖ **SIM - Adicione:**
- Qualquer erro que impediu progresso
- Warnings que causaram confus√£o
- Problemas que levaram >5 min para resolver
- Erros n√£o documentados em troubleshooting.md original

‚ùå **N√ÉO - N√£o adicione:**
- Erros por typo √≥bvio (esqueceu v√≠rgula, etc)
- Problemas j√° documentados no troubleshooting.md
- Issues de infraestrutura externa (GitHub down, AWS outage)

### Como usar este documento?

**Durante implementa√ß√£o:**
1. Encontrou problema ‚Üí Documente IMEDIATAMENTE
2. N√£o espere resolver para documentar
3. Use template acima
4. Seja ESPEC√çFICO (comandos exatos, erros completos)

**Antes de nova implementa√ß√£o:**
1. Leia todos problemas desta categoria
2. Execute valida√ß√µes preventivas sugeridas
3. Evite repetir erros j√° solucionados

**Para melhorar documenta√ß√£o principal:**
- Problemas recorrentes ‚Üí Adicionar ao guia principal
- Problemas com solu√ß√£o r√°pida ‚Üí Adicionar ao troubleshooting.md
- Problemas de design ‚Üí Considerar ajuste na arquitetura

---

## üîç √çNDICE DE PROBLEMAS (Atualizar conforme adicionar)

### Por Severidade
- üî¥ Bloqueantes: 0
- üü° Warnings: 0
- üü¢ Leves: 0

### Por Fase
- **Setup**: 0
- **Config**: 0
- **Terraform**: 0
- **Install**: 0
- **Deploy**: 0
- **Cleanup**: 0

### Por Tipo
- **Limites AWS**: 0
- **Networking**: 0
- **Permiss√µes IAM**: 0
- **Kubernetes**: 0
- **DNS/Certificados**: 0
- **GitHub**: 0
- **Custo**: 0
- **Performance**: 0

---

**√öltima atualiza√ß√£o:** 2024-12-09
**Total problemas documentados:** 0 (3 exemplos template)
**Implementa√ß√µes bem-sucedidas:** 0
