# üìä An√°lise T√©cnica Detalhada: IDP AWS

> **P√∫blico-alvo**: Arquitetos, SREs, Engenheiros Senior
> **Objetivo**: Deep dive t√©cnico, otimiza√ß√µes, melhorias e roadmap

---

## üìë √çNDICE

1. [Arquitetura Atual](#1-arquitetura-atual)
2. [Compara√ß√£o com AWS Well-Architected](#2-compara√ß√£o-com-aws-well-architected)
3. [An√°lise de Seguran√ßa](#3-an√°lise-de-seguran√ßa)
4. [An√°lise de Custos Detalhada](#4-an√°lise-de-custos-detalhada)
5. [Escalabilidade e Performance](#5-escalabilidade-e-performance)
6. [Plano de Testes](#6-plano-de-testes)
7. [Melhorias Prioritizadas](#7-melhorias-prioritizadas)

---

## 1. ARQUITETURA ATUAL

### 1.1 Diagrama de Arquitetura Detalhado

```mermaid
graph TB
    subgraph "User Layer"
        DEV[üë®‚Äçüíª Developer]
        OPS[üë®‚Äçüîß Platform Engineer]
    end

    subgraph "Frontend Layer"
        BACKSTAGE[Backstage<br/>Port 80/443]
        ARGOCD[ArgoCD UI<br/>Port 80/443]
    end

    subgraph "AWS us-east-1"
        R53[Route 53<br/>DNS Resolution]
        ACM[Certificate Manager<br/>TLS Certificates]

        subgraph "VPC 10.0.0.0/16"
            subgraph "Public Subnets (10.0.0.0/19)"
                IGW[Internet Gateway]
                NAT[NAT Gateway<br/>us-east-1a]
                ALB[Application LB<br/>Target: NGINX]
            end

            subgraph "Private Subnets (10.0.32.0/19)"
                subgraph "EKS Cluster"
                    CONTROL[EKS Control Plane<br/>Managed by AWS]

                    subgraph "Data Plane - Spot Nodes"
                        NODE1[t3.medium Spot<br/>us-east-1a]
                        NODE2[t3.medium Spot<br/>us-east-1b]
                    end

                    subgraph "Core Platform - Namespace: System"
                        NGINX_POD[NGINX Ingress<br/>DaemonSet]
                        CERTMGR[Cert Manager<br/>Deployment]
                        EXTDNS[External DNS<br/>Deployment]
                        EXTSEC[External Secrets<br/>Deployment]
                    end

                    subgraph "GitOps - Namespace: argocd"
                        ARGOCD_SERVER[ArgoCD Server<br/>Deployment]
                        ARGOCD_REPO[Repo Server<br/>StatefulSet]
                        ARGOCD_APP[Application Controller<br/>StatefulSet]
                    end

                    subgraph "Developer Portal - Namespace: backstage"
                        BACKSTAGE_POD[Backstage Frontend<br/>Deployment]
                        BACKSTAGE_DB[PostgreSQL<br/>StatefulSet]
                    end

                    subgraph "Identity - Namespace: keycloak"
                        KEYCLOAK_POD[Keycloak<br/>StatefulSet]
                        KC_DB[PostgreSQL<br/>StatefulSet]
                    end

                    subgraph "IaC - Namespace: crossplane"
                        CROSSPLANE[Crossplane Core<br/>Deployment]
                        CP_AWS[AWS Provider<br/>Deployment]
                    end
                end
            end
        end

        subgraph "AWS Services"
            SM[Secrets Manager<br/>2 secrets]
            CW[CloudWatch Logs<br/>EKS logs]
            EBS[EBS Volumes<br/>gp3 100GB]
            IAM_ROLES[IAM Roles<br/>Pod Identity]
        end
    end

    subgraph "External"
        GH[GitHub<br/>Source Control]
        LE[Let's Encrypt<br/>ACME CA]
    end

    DEV --> BACKSTAGE
    OPS --> ARGOCD

    DEV --> R53
    R53 --> ALB
    ALB --> NGINX_POD
    NGINX_POD --> BACKSTAGE_POD
    NGINX_POD --> ARGOCD_SERVER

    CERTMGR --> LE
    EXTDNS --> R53
    EXTSEC --> SM

    BACKSTAGE_POD --> GH
    ARGOCD_REPO --> GH

    CROSSPLANE --> IAM_ROLES
    CP_AWS --> AWS_API[AWS APIs]

    NODE1 --> EBS
    NODE2 --> EBS

    CONTROL --> CW

    classDef user fill:#4CAF50,stroke:#2E7D32,color:white
    classDef frontend fill:#2196F3,stroke:#1565C0,color:white
    classDef aws fill:#FF9900,stroke:#E65100,color:white
    classDef k8s fill:#326CE5,stroke:#1565C0,color:white
    classDef external fill:#9E9E9E,stroke:#424242,color:white

    class DEV,OPS user
    class BACKSTAGE,ARGOCD frontend
    class R53,ACM,ALB,NAT,IGW,SM,CW,EBS,IAM_ROLES aws
    class CONTROL,NODE1,NODE2,NGINX_POD,CERTMGR,EXTDNS,EXTSEC,ARGOCD_SERVER,BACKSTAGE_POD,KEYCLOAK_POD,CROSSPLANE k8s
    class GH,LE external
```

### 1.2 Componentes e Tecnologias

#### Infraestrutura AWS

| Componente | Tipo | Especifica√ß√£o | Justificativa |
|------------|------|---------------|---------------|
| **EKS** | Managed K8s | v1.33 | Reduz overhead operacional vs self-managed |
| **VPC** | Network | 10.0.0.0/16 (65536 IPs) | Espa√ßo suficiente para crescimento |
| **Subnets** | Network | 3 AZs (public + private) | Alta disponibilidade |
| **NAT Gateway** | Network | Single AZ | Tradeoff: custo vs HA |
| **ALB** | Load Balancer | Application LB | L7 routing, integra√ß√£o com EKS |
| **EC2 Spot** | Compute | t3.medium (2 vCPU, 4GB RAM) | 70% economia vs on-demand |
| **EBS gp3** | Storage | 100GB, 3000 IOPS | Melhor custo-benef√≠cio vs gp2 |

#### Kubernetes Addons

| Addon | Vers√£o | Namespace | Pod Count | Resources |
|-------|--------|-----------|-----------|-----------|
| **ArgoCD** | 2.9.x | argocd | 7 pods | 500m CPU, 1Gi RAM |
| **Backstage** | 1.21.x | backstage | 2 pods | 1000m CPU, 2Gi RAM |
| **Keycloak** | 24.x | keycloak | 2 pods | 500m CPU, 1Gi RAM |
| **Crossplane** | 1.20.x | crossplane-system | 2 pods | 500m CPU, 1Gi RAM |
| **Cert Manager** | 1.17.x | cert-manager | 3 pods | 300m CPU, 512Mi RAM |
| **External DNS** | 0.14.x | external-dns | 1 pod | 100m CPU, 256Mi RAM |
| **External Secrets** | 0.9.x | external-secrets | 2 pods | 200m CPU, 512Mi RAM |
| **NGINX Ingress** | 1.9.x | ingress-nginx | 2 pods | 200m CPU, 512Mi RAM |

**Total Resources Required:**
- CPU: ~3.3 vCPU
- Memory: ~6.5 GB
- Storage: ~10 GB (PVCs)

**Node Capacity (2x t3.medium):**
- CPU: 4 vCPU total (ap√≥s system pods: ~3.2 dispon√≠vel)
- Memory: 8 GB total (ap√≥s system pods: ~6.5 dispon√≠vel)
- **Utiliza√ß√£o**: ~80% (√≥timo para POC, ajustar para prod)

### 1.3 Fluxo de Rede e Seguran√ßa

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INGRESS FLOW (User ‚Üí App)                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                            ‚îÇ
‚îÇ  1. User browser: https://backstage.timedevops.click      ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  2. DNS Query ‚Üí Route 53                                  ‚îÇ
‚îÇ     Response: ALB IP (52.x.x.x)                          ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  3. TLS Handshake                                         ‚îÇ
‚îÇ     Certificate: Let's Encrypt (from Cert Manager)        ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  4. ALB ‚Üí Target Group (NGINX Ingress NodePort 30080)     ‚îÇ
‚îÇ     Security Group: Allow 80/443 from 0.0.0.0/0          ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  5. NGINX Ingress Controller                              ‚îÇ
‚îÇ     TLS Termination (optional, pode ser no ALB)           ‚îÇ
‚îÇ     Host-based routing                                    ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  6. Kubernetes Service: backstage.backstage.svc           ‚îÇ
‚îÇ     Type: ClusterIP                                       ‚îÇ
‚îÇ     Port: 7007                                            ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  7. Backstage Pod                                         ‚îÇ
‚îÇ     Container Port: 7007                                  ‚îÇ
‚îÇ                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  EGRESS FLOW (App ‚Üí External)                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                            ‚îÇ
‚îÇ  1. Pod needs external API (GitHub, AWS, etc)             ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  2. Route Table ‚Üí NAT Gateway                             ‚îÇ
‚îÇ     Private subnet default route: 0.0.0.0/0 ‚Üí NAT        ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  3. NAT Gateway (in public subnet)                        ‚îÇ
‚îÇ     SNAT: Pod IP ‚Üí NAT Gateway Elastic IP                ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  4. Internet Gateway                                      ‚îÇ
‚îÇ     ‚Üì                                                      ‚îÇ
‚îÇ  5. External destination (GitHub: 140.82.x.x)             ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ  üí∞ Custo: NAT Gateway = $0.045/GB transferred            ‚îÇ
‚îÇ                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.4 Decis√µes de Design e Justificativas

#### 1. Por que Kubernetes e n√£o ECS/Fargate?

| Crit√©rio | EKS | ECS Fargate | Decis√£o |
|----------|-----|-------------|---------|
| **Portabilidade** | ‚úÖ Multi-cloud | ‚ùå AWS only | EKS |
| **Ecossistema** | ‚úÖ Enorme (Helm, operators) | ‚ö†Ô∏è Limitado | EKS |
| **Controle** | ‚úÖ Completo | ‚ö†Ô∏è Abstra√ß√£o alta | EKS |
| **Custo** | ‚ö†Ô∏è Gerenciar n√≥s | ‚úÖ Pay-per-task | Depende |
| **Curva aprendizado** | ‚ùå Alta | ‚úÖ Baixa | EKS (padr√£o ind√∫stria) |

**Conclus√£o**: EKS escolhido por **portabilidade** e **ecossistema maduro de IDPs**.

#### 2. Por que GitOps (ArgoCD) e n√£o CI/CD tradicional?

**Vantagens GitOps:**
- ‚úÖ Git como fonte √∫nica da verdade (auditoria completa)
- ‚úÖ Rollback trivial (git revert)
- ‚úÖ Drift detection (cluster vs Git)
- ‚úÖ Multi-cluster management f√°cil
- ‚úÖ Declarativo > Imperativo

**Trade-off**: Curva de aprendizado inicial mais alta.

#### 3. Por que Spot Instances?

```
On-Demand t3.medium: $0.0416/h
Spot t3.medium:      $0.0125/h (70% desconto)

Savings: $730/m√™s para cluster 2 n√≥s

Risco: Spot pode ser terminado com 2 min aviso
Mitiga√ß√£o:
  ‚îú‚îÄ Diversificar instance types (t3.medium, t3a.medium)
  ‚îú‚îÄ Spread across AZs
  ‚îú‚îÄ Cluster Autoscaler com fallback on-demand
  ‚îî‚îÄ PodDisruptionBudgets para apps cr√≠ticos
```

**Para POC**: Spot √© perfeito (custo > HA)
**Para Prod**: Mix 50% on-demand + 50% spot

#### 4. Por que Single NAT Gateway?

```
Multi-AZ NAT (3 gateways): $96/m√™s
Single NAT Gateway:         $32/m√™s
Economia: $64/m√™s (67%)

Risco: Se AZ do NAT cai, egress para (mas ingress via ALB OK)
Mitiga√ß√£o para Prod: Multi-AZ NAT
```

---

## 2. COMPARA√á√ÉO COM AWS WELL-ARCHITECTED

### 2.1 Pilar: Excel√™ncia Operacional

| Best Practice | Status | Implementa√ß√£o | Gap |
|---------------|--------|---------------|-----|
| **IaC para tudo** | ‚úÖ | Terraform + Helm | - |
| **CI/CD automatizado** | ‚úÖ | GitOps (ArgoCD) | - |
| **Observabilidade** | ‚ö†Ô∏è | CloudWatch b√°sico | Prometheus/Grafana faltando |
| **Runbooks documentados** | ‚ùå | Apenas README | Criar runbooks operacionais |
| **Chaos engineering** | ‚ùå | N√£o implementado | Adicionar Litmus/Chaos Mesh |
| **Disaster recovery testado** | ‚ùå | Backup n√£o testado | Gamedays trimestrais |

**Score**: 5/10 (POC), Target Prod: 9/10

### 2.2 Pilar: Seguran√ßa

| Best Practice | Status | Implementa√ß√£o | Gap |
|---------------|--------|---------------|-----|
| **IAM least privilege** | ‚úÖ | Pod Identity granular | - |
| **Secrets management** | ‚úÖ | External Secrets + AWS SM | - |
| **Network segmentation** | ‚ö†Ô∏è | Security Groups | Network Policies K8s faltando |
| **Encryption at rest** | ‚úÖ | EBS encrypted | - |
| **Encryption in transit** | ‚úÖ | TLS everywhere | - |
| **Vulnerability scanning** | ‚ùå | N√£o configurado | Adicionar Trivy/Snyk |
| **RBAC K8s** | ‚ö†Ô∏è | B√°sico | Refinar permiss√µes |
| **Audit logging** | ‚úÖ | CloudTrail + EKS logs | - |
| **WAF** | ‚ùå | N√£o configurado | Adicionar AWS WAF |

**Score**: 6/10 (POC), Target Prod: 9/10

### 2.3 Pilar: Confiabilidade

| Best Practice | Status | Implementa√ß√£o | Gap |
|---------------|--------|---------------|-----|
| **Multi-AZ** | ‚ö†Ô∏è | N√≥s sim, NAT n√£o | Single NAT = SPOF |
| **Auto-healing** | ‚úÖ | K8s liveness/readiness | - |
| **Auto-scaling** | ‚ö†Ô∏è | HPA manual | Cluster Autoscaler falta |
| **Backups** | ‚ùå | N√£o configurado | Velero para backups |
| **RTO < 1h** | ‚ùå | N√£o testado | DR plan + testes |
| **RPO < 1h** | ‚ùå | N√£o configurado | Backups cont√≠nuos |
| **Health checks** | ‚úÖ | K8s probes | - |

**Score**: 4/10 (POC), Target Prod: 9/10

### 2.4 Pilar: Efici√™ncia de Performance

| Best Practice | Status | Implementa√ß√£o | Gap |
|---------------|--------|---------------|-----|
| **Resource limits** | ‚ö†Ô∏è | Alguns pods | Padronizar todos |
| **Horizontal scaling** | ‚ö†Ô∏è | HPA configurado | Testar limites |
| **CDN** | ‚ùå | N√£o configurado | CloudFront para static |
| **Caching** | ‚ö†Ô∏è | Apenas app-level | Adicionar Redis/Elasticache |
| **Database indexing** | ‚ö†Ô∏è | PostgreSQL defaults | Otimizar queries |
| **Load testing** | ‚ùå | N√£o realizado | K6/Locust |

**Score**: 4/10 (POC), Target Prod: 8/10

### 2.5 Pilar: Otimiza√ß√£o de Custos

| Best Practice | Status | Implementa√ß√£o | Gap |
|---------------|--------|---------------|-----|
| **Spot instances** | ‚úÖ | 100% spot POC | - |
| **Rightsizing** | ‚ö†Ô∏è | Estimado | Analisar m√©tricas reais |
| **Cost allocation tags** | ‚úÖ | Todas resources | - |
| **Savings Plans** | ‚ùå | N√£o aplic√°vel POC | Considerar para prod |
| **Budget alerts** | ‚úÖ | Configurado | - |
| **Storage tiering** | ‚ùå | Apenas gp3 | S3 lifecycle para backups |
| **Idle resource detection** | ‚ùå | Manual | Automatizar com lambdas |

**Score**: 5/10 (POC), Target Prod: 8/10

### 2.6 Pilar: Sustentabilidade

| Best Practice | Status | Implementa√ß√£o | Gap |
|---------------|--------|---------------|-----|
| **Minimize overprovisioning** | ‚úÖ | Spot + small instances | - |
| **Desligar ambientes dev/noite** | ‚ùå | Manual | Automatizar com Lambda |
| **Usar graviton (ARM)** | ‚ùå | x86 | Considerar t4g (20% economia) |
| **Medir carbon footprint** | ‚ùå | N√£o rastreado | AWS Carbon Footprint Tool |

**Score**: 3/10

---

## 3. AN√ÅLISE DE SEGURAN√áA

### 3.1 Modelo de Amea√ßas (STRIDE)

| Amea√ßa | Vetor de Ataque | Impacto | Mitiga√ß√£o Atual | Gap |
|--------|-----------------|---------|-----------------|-----|
| **Spoofing** | Falso GitHub webhook | Alto | TLS + GitHub App signature | ‚úÖ OK |
| **Tampering** | Modificar manifests no Git | Cr√≠tico | Branch protection | ‚ö†Ô∏è Melhorar |
| **Repudiation** | Negar a√ß√µes maliciosas | M√©dio | CloudTrail + EKS audit | ‚úÖ OK |
| **Info Disclosure** | Secrets vazados | Cr√≠tico | External Secrets | ‚ö†Ô∏è Rotation falta |
| **DoS** | Sobrecarga ALB/pods | Alto | Rate limiting b√°sico | ‚ùå WAF falta |
| **Privilege Escalation** | Pod escape | Cr√≠tico | SecurityContext | ‚ö†Ô∏è PSP/PSA falta |

### 3.2 Configura√ß√µes de Seguran√ßa K8s

#### Pod Security Standards (PSS)

**Atual**: Permissive (padr√£o K8s)

**Recomendado para Prod**:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: backstage
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

#### Network Policies

**Atual**: ‚ùå Nenhuma (todo pod acessa todo pod)

**Recomendado**:

```yaml
# Exemplo: Isolar Backstage
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backstage-netpol
  namespace: backstage
spec:
  podSelector:
    matchLabels:
      app: backstage
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 7007
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              name: keycloak
      ports:
        - protocol: TCP
          port: 8080
    - to:  # GitHub
        - ipBlock:
            cidr: 0.0.0.0/0
      ports:
        - protocol: TCP
          port: 443
```

### 3.3 Secrets Management

#### Rota√ß√£o de Secrets

```bash
# Atual: Secrets nunca rotacionam
# Risco: Credenciais comprometidas permanecem v√°lidas

# Solu√ß√£o: Automatizar rota√ß√£o
# 1. GitHub Apps: Regenerar private keys trimestralmente
# 2. Keycloak client secrets: Renovar mensalmente
# 3. Database passwords: AWS Secrets Manager auto-rotation
```

#### Scan de Secrets em Git

```bash
# Ferramenta: gitleaks
# CI/CD check antes de merge

# .github/workflows/secrets-scan.yml
name: Secrets Scan
on: [push]
jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Gitleaks
        uses: gitleaks/gitleaks-action@v2
```

### 3.4 Compliance

#### LGPD (Lei Geral de Prote√ß√£o de Dados)

| Requisito | Implementa√ß√£o | Status |
|-----------|---------------|--------|
| **Dados em regi√£o espec√≠fica** | VPC us-east-1 | ‚úÖ Configur√°vel |
| **Criptografia** | EBS + TLS | ‚úÖ |
| **Audit trail** | CloudTrail | ‚úÖ |
| **Right to erasure** | Manual | ‚ùå Automatizar |
| **Consent management** | N√£o aplic√°vel (IDP interno) | N/A |
| **Data breach notification** | Alertas CloudWatch | ‚ö†Ô∏è Melhorar |

#### SOC 2

| Control | Status | Evid√™ncia |
|---------|--------|-----------|
| **Access Control** | ‚ö†Ô∏è | RBAC b√°sico configurado |
| **Change Management** | ‚úÖ | GitOps = audit√°vel |
| **System Monitoring** | ‚ö†Ô∏è | CloudWatch, falta alertas proativos |
| **Risk Assessment** | ‚ùå | N√£o documentado |
| **Vendor Management** | ‚ö†Ô∏è | Depend√™ncias n√£o auditadas |

---

## 4. AN√ÅLISE DE CUSTOS DETALHADA

### 4.1 Breakdown por Servi√ßo (POC 24/7)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMPUTE                                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  EKS Control Plane (1 cluster)                          ‚îÇ
‚îÇ    $0.10/h √ó 730h = $73.00                             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  EC2 Spot Instances (2x t3.medium)                      ‚îÇ
‚îÇ    $0.0125/h √ó 730h √ó 2 = $18.25                       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Data Transfer (NAT Gateway egress)                     ‚îÇ
‚îÇ    ~50 GB/m√™s √ó $0.045/GB = $2.25                      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  SUBTOTAL COMPUTE: $93.50/m√™s                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STORAGE                                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  EBS gp3 (2 nodes √ó 50GB)                               ‚îÇ
‚îÇ    100GB √ó $0.08/GB = $8.00                            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  EBS Snapshots (backups)                                ‚îÇ
‚îÇ    50GB √ó $0.05/GB = $2.50                             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  SUBTOTAL STORAGE: $10.50/m√™s                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NETWORKING                                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  NAT Gateway (single AZ)                                ‚îÇ
‚îÇ    $0.045/h √ó 730h = $32.85                            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Application Load Balancer                              ‚îÇ
‚îÇ    $0.0225/h √ó 730h = $16.43                           ‚îÇ
‚îÇ    LCU charges: ~$3/m√™s                                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Route 53 Hosted Zone                                   ‚îÇ
‚îÇ    $0.50/zone                                          ‚îÇ
‚îÇ    Queries: 1M √ó $0.40 = $0.40                         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  SUBTOTAL NETWORKING: $53.18/m√™s                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SERVICES                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Secrets Manager (2 secrets)                            ‚îÇ
‚îÇ    2 √ó $0.40 = $0.80                                   ‚îÇ
‚îÇ    API calls: 10k √ó $0.05/10k = $0.05                  ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  CloudWatch Logs (5 GB/m√™s)                             ‚îÇ
‚îÇ    Ingestion: 5GB √ó $0.50/GB = $2.50                   ‚îÇ
‚îÇ    Storage: 5GB √ó $0.03/GB = $0.15                     ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  CloudWatch Alarms (5 alarms)                           ‚îÇ
‚îÇ    5 √ó $0.10 = $0.50                                   ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  SUBTOTAL SERVICES: $4.00/m√™s                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TOTAL MENSAL (POC 24/7)                                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Compute:     $93.50                                   ‚îÇ
‚îÇ  Storage:     $10.50                                   ‚îÇ
‚îÇ  Networking:  $53.18                                   ‚îÇ
‚îÇ  Services:    $4.00                                    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                     ‚îÇ
‚îÇ  TOTAL:       $161.18/m√™s                              ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Se rodar apenas 8h/dia √∫til (160h/m√™s):              ‚îÇ
‚îÇ  - Economiza em EC2 spot: ~$13                         ‚îÇ
‚îÇ  - EKS Control Plane continua $73 (n√£o para)           ‚îÇ
‚îÇ  = Total: ~$148/m√™s                                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 Otimiza√ß√µes de Custo

#### Quick Wins (Implementar J√°)

```
1. Usar Savings Plans (Prod apenas)
   ‚îî‚îÄ> 1 year commit: 20% desconto EKS
   ‚îî‚îÄ> Economia: $15/m√™s

2. Reserved Capacity NAT Gateway
   ‚îî‚îÄ> 1 year commit: 15% desconto
   ‚îî‚îÄ> Economia: $5/m√™s

3. Graviton instances (t4g ao inv√©s de t3)
   ‚îî‚îÄ> 20% mais barato + performance
   ‚îî‚îÄ> Economia: $4/m√™s

4. Reduzir reten√ß√£o CloudWatch Logs (30d ‚Üí 7d)
   ‚îî‚îÄ> Economia: $2/m√™s

5. Usar S3 para backups ao inv√©s de EBS snapshots
   ‚îî‚îÄ> EBS: $0.05/GB vs S3: $0.023/GB
   ‚îî‚îÄ> Economia: $1.35/m√™s

TOTAL Quick Wins: ~$27/m√™s (17% redu√ß√£o)
```

#### M√©dio Prazo (1-3 meses)

```
6. VPC Endpoints para AWS Services
   ‚îî‚îÄ> Evitar NAT Gateway egress charges
   ‚îî‚îÄ> Economia: $15-30/m√™s (depende tr√°fego)

7. Spot Instance Autoscaler inteligente
   ‚îî‚îÄ> Scale to zero quando n√£o usar
   ‚îî‚îÄ> Economia: $50-100/m√™s (dev environment)

8. FinOps Dashboard
   ‚îî‚îÄ> Visibilidade = economia 10-15%
   ‚îî‚îÄ> Economia: $16-24/m√™s

9. Kubernetes Resource Requests otimizados
   ‚îî‚îÄ> Reduzir over-provisioning
   ‚îî‚îÄ> Economia: Reduzir 1 n√≥ = $9/m√™s

TOTAL M√©dio Prazo: ~$90-163/m√™s adicional
```

### 4.3 Cost Allocation por Namespace

```bash
# Implementar tags em todos resources K8s
# Permite rastrear custo por team/projeto

kubectl label namespace backstage \
  cost-center=engineering \
  team=platform \
  project=idp

# Depois an√°lise no Cost Explorer:
aws ce get-cost-and-usage \
  --time-period Start=2024-12-01,End=2024-12-31 \
  --granularity MONTHLY \
  --metrics "UnblendedCost" \
  --group-by Type=TAG,Key=team
```

---

## 5. ESCALABILIDADE E PERFORMANCE

### 5.1 Limites Atuais

| Recurso | Limite Atual | Gargalo | Quando Atingir |
|---------|--------------|---------|----------------|
| **Pods** | ~100 pods | 2 n√≥s t3.medium | 50 apps |
| **CPU** | 4 vCPU total | Compute | 30 apps |
| **Memory** | 8 GB total | Compute | 30 apps |
| **IPs** | 8 IPs/n√≥ | ENI limits | 16 pods/n√≥ |
| **ALB connections** | 25 rules | ALB limit | 25 apps |
| **Route53 queries** | Ilimitado | - | Nunca |

### 5.2 Como Escalar Horizontalmente

#### Adicionar Mais N√≥s (Manual)

```bash
cd cluster/terraform

# Editar main.tf
# Aumentar desired_size de 2 para 4

  eks_managed_node_groups = {
    spot_nodes = {
      min_size     = 2
      max_size     = 6
      desired_size = 4  # ‚Üê Aumentar aqui
    }
  }

terraform apply
```

#### Cluster Autoscaler (Autom√°tico - Recomendado Prod)

```yaml
# packages/cluster-autoscaler/values.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.0
        name: cluster-autoscaler
        command:
          - ./cluster-autoscaler
          - --v=4
          - --stderrthreshold=info
          - --cloud-provider=aws
          - --skip-nodes-with-local-storage=false
          - --expander=least-waste
          - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/idp-poc-cluster
          - --balance-similar-node-groups
          - --skip-nodes-with-system-pods=false
```

### 5.3 Performance Tuning

#### Backstage Response Time

```
Atual (sem otimiza√ß√£o):
  ‚îú‚îÄ TTFB (Time to First Byte): 800ms
  ‚îú‚îÄ Full page load: 2.1s
  ‚îî‚îÄ API calls: 300-500ms

Otimiza√ß√µes:
  1. Redis cache para Backstage catalog
     ‚îî‚îÄ> TTFB: 800ms ‚Üí 200ms

  2. CDN (CloudFront) para static assets
     ‚îî‚îÄ> Page load: 2.1s ‚Üí 900ms

  3. PostgreSQL connection pooling (PgBouncer)
     ‚îî‚îÄ> API: 400ms ‚Üí 150ms

  4. Horizontal Pod Autoscaler (HPA)
     ‚îî‚îÄ> Manter latency < 500ms sob carga
```

#### ArgoCD Sync Performance

```
Problema: 100+ apps = sync lento

Solu√ß√µes:
  1. Aumentar --repo-server-replicas=3
  2. Usar ApplicationSet com waves
  3. Enable ResourceTracking (reduz API calls)
  4. Usar Webhook ao inv√©s de polling

Resultado:
  ‚îú‚îÄ Antes: Sync 100 apps = 30 min
  ‚îî‚îÄ Depois: Sync 100 apps = 8 min
```

### 5.4 Testes de Carga

```bash
# Ferramenta: K6

# test-backstage-load.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 10 },   # Warm-up
    { duration: '5m', target: 50 },   # Ramp-up
    { duration: '10m', target: 100 }, # Peak
    { duration: '2m', target: 0 },    # Ramp-down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'], # 95% requests < 500ms
    http_req_failed: ['rate<0.01'],   # Error rate < 1%
  },
};

export default function () {
  const res = http.get('https://backstage.timedevops.click');
  check(res, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
  sleep(1);
}

# Executar:
k6 run test-backstage-load.js
```

---

## 6. PLANO DE TESTES

### 6.1 Matriz de Testes

| Tipo | Cobertura | Ferramenta | Frequ√™ncia |
|------|-----------|------------|------------|
| **Unit Tests** | Backstage plugins | Jest | A cada commit |
| **Integration Tests** | APIs E2E | Postman/Newman | A cada PR |
| **UI Tests** | Backstage flows | Playwright | A cada release |
| **Security Tests** | Vulns, secrets | Trivy, GitLeaks | Diariamente |
| **Performance Tests** | Load, stress | K6 | Semanalmente |
| **Chaos Tests** | Failure scenarios | Litmus | Mensalmente |
| **Disaster Recovery** | Backup/restore | Manual | Trimestralmente |

### 6.2 Smoke Tests (Deploy Validation)

```bash
#!/bin/bash
# tests/smoke-test.sh

set -e

echo "üîç Running smoke tests..."

# Test 1: All pods running
echo "1. Checking pod health..."
kubectl wait --for=condition=Ready pods --all -n backstage --timeout=300s
kubectl wait --for=condition=Ready pods --all -n argocd --timeout=300s

# Test 2: Ingress accessible
echo "2. Testing ingress endpoints..."
curl -f -s -o /dev/null https://backstage.timedevops.click || exit 1
curl -f -s -o /dev/null https://argocd.timedevops.click || exit 1

# Test 3: ArgoCD apps healthy
echo "3. Checking ArgoCD application health..."
UNHEALTHY=$(kubectl get applications -n argocd -o json | \
  jq -r '.items[] | select(.status.health.status != "Healthy") | .metadata.name')

if [ -n "$UNHEALTHY" ]; then
  echo "‚ùå Unhealthy apps: $UNHEALTHY"
  exit 1
fi

# Test 4: Can create app via Backstage API
echo "4. Testing Backstage API..."
TOKEN=$(kubectl get secret -n backstage backstage-backend-secret \
  -o jsonpath='{.data.token}' | base64 -d)

curl -f -H "Authorization: Bearer $TOKEN" \
  https://backstage.timedevops.click/api/catalog/entities?filter=kind=Component \
  || exit 1

echo "‚úÖ All smoke tests passed!"
```

### 6.3 Chaos Engineering Scenarios

```yaml
# chaos-experiments/pod-delete.yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosEngine
metadata:
  name: backstage-chaos
  namespace: backstage
spec:
  appinfo:
    appns: backstage
    applabel: app=backstage
  engineState: active
  chaosServiceAccount: litmus-admin
  experiments:
    - name: pod-delete
      spec:
        components:
          env:
            - name: TOTAL_CHAOS_DURATION
              value: "60" # 60 seconds
            - name: CHAOS_INTERVAL
              value: "10"
            - name: FORCE
              value: "false"
            - name: PODS_AFFECTED_PERC
              value: "50" # Kill 50% of pods

# Valida√ß√£o:
# - Backstage deve continuar acess√≠vel (HPA scale)
# - Latency < 1s durante chaos
# - Sem erros no frontend
```

---

## 7. MELHORIAS PRIORITIZADAS

### 7.1 Quick Wins (at√© 1 semana)

#### 1. Adicionar Resource Limits em Todos Pods

**Problema**: Pods sem limits podem consumir todos recursos do n√≥.

```yaml
# Exemplo: packages/backstage/values.yaml
resources:
  requests:
    cpu: 500m
    memory: 1Gi
  limits:
    cpu: 1000m
    memory: 2Gi
```

**Impacto**: Previne noisy neighbor, melhora scheduling
**Esfor√ßo**: 4 horas
**Prioridade**: üî¥ Alta

#### 2. Configurar PodDisruptionBudgets

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: backstage-pdb
  namespace: backstage
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: backstage
```

**Impacto**: Garante disponibilidade durante upgrades
**Esfor√ßo**: 2 horas
**Prioridade**: üî¥ Alta

#### 3. Habilitar Cluster Autoscaler

**Impacto**: Escala autom√°tica = economia + performance
**Esfor√ßo**: 6 horas
**Prioridade**: üü° M√©dia (Prod only)

#### 4. Configurar Alertas Proativos

```yaml
# CloudWatch Alarms
alarms:
  - name: high-cpu-utilization
    threshold: 80%
    duration: 5min
    action: SNS topic

  - name: pod-crash-loop
    threshold: 3 restarts
    duration: 5min
    action: PagerDuty

  - name: argocd-sync-failed
    threshold: 1 failure
    duration: 1min
    action: Slack
```

**Impacto**: Detectar problemas antes de impactar usu√°rios
**Esfor√ßo**: 8 horas
**Prioridade**: üî¥ Alta

---

### 7.2 M√©dio Prazo (1-3 meses)

#### 5. Implementar Service Mesh (Istio)

**Benef√≠cios**:
- Observabilidade avan√ßada (distributed tracing)
- Circuit breakers autom√°ticos
- mTLS between services
- Canary deployments f√°ceis

**Trade-off**: Complexidade +30%, latency +5ms
**Esfor√ßo**: 3 semanas
**Prioridade**: üü° M√©dia

#### 6. FinOps Dashboard (Kubecost)

```bash
helm install kubecost kubecost/cost-analyzer \
  --namespace kubecost \
  --set kubecostToken="<FREE_TOKEN>"

# Acesso: https://kubecost.timedevops.click
```

**Benef√≠cios**:
- Custo por namespace/team/app
- Recomenda√ß√µes de rightsizing
- Budget alerts granulares

**Esfor√ßo**: 1 semana
**Prioridade**: üü° M√©dia

#### 7. Disaster Recovery Automation (Velero)

```bash
# Backup di√°rio autom√°tico
velero schedule create daily-backup \
  --schedule="0 2 * * *" \
  --include-namespaces backstage,argocd,keycloak

# Restore test mensal
velero restore create --from-backup daily-backup-20241209
```

**Esfor√ßo**: 2 semanas
**Prioridade**: üü° M√©dia (Prod only)

---

### 7.3 Longo Prazo (3-6 meses)

#### 8. Multi-Cluster Management

```
Arquitetura Target:
‚îú‚îÄ Hub Cluster (gerenciamento)
‚îÇ  ‚îî‚îÄ ArgoCD central
‚îÇ
‚îú‚îÄ Dev Cluster (us-east-1)
‚îú‚îÄ Staging Cluster (us-east-1)
‚îî‚îÄ Prod Cluster (multi-region)
   ‚îú‚îÄ us-east-1 (primary)
   ‚îî‚îÄ us-west-2 (DR)
```

**Benef√≠cios**: Isolamento, multi-tenancy
**Esfor√ßo**: 2 meses
**Prioridade**: üü¢ Baixa

#### 9. Self-Healing Avan√ßado

```
Implementar:
‚îú‚îÄ Auto-remediation de alertas comuns
‚îú‚îÄ Rollback autom√°tico em falhas
‚îú‚îÄ Auto-scaling predictive (ML)
‚îî‚îÄ Chaos engineering cont√≠nuo
```

**Esfor√ßo**: 3 meses
**Prioridade**: üü¢ Baixa

#### 10. Developer Metrics & Analytics

```
M√©tricas a rastrear:
‚îú‚îÄ DORA metrics (deployment frequency, lead time, MTTR, change fail rate)
‚îú‚îÄ Backstage adoption (DAU, templates usados)
‚îú‚îÄ Self-service rate (% requests sem tickets)
‚îî‚îÄ Developer satisfaction (NPS)
```

**Esfor√ßo**: 6 semanas
**Prioridade**: üü¢ Baixa

---

## üìã ROADMAP T√âCNICO (12 MESES)

```mermaid
gantt
    title Technical Roadmap IDP AWS
    dateFormat YYYY-MM-DD
    section Quick Wins
    Resource Limits          :2024-12-10, 1d
    PodDisruptionBudgets     :2024-12-11, 1d
    Proactive Alerts         :2024-12-12, 2d

    section M√©dio Prazo
    Cluster Autoscaler       :2024-12-15, 1w
    FinOps Dashboard         :2024-12-20, 1w
    Network Policies         :2025-01-05, 2w
    Service Mesh (Istio)     :2025-01-20, 3w
    Velero Backups           :2025-02-10, 2w

    section Longo Prazo
    Multi-Cluster            :2025-03-01, 2M
    Advanced Observability   :2025-05-01, 6w
    Self-Healing             :2025-06-15, 3M
    ML-based Optimization    :2025-09-01, 3M
```

---

## üéØ CONCLUS√ÉO E RECOMENDA√á√ïES

### Para POC (Atual)

‚úÖ **Arquitetura adequada para valida√ß√£o**
- Custo otimizado (Spot instances)
- Funcionalidade completa
- F√°cil destruir e reconstruir

‚ö†Ô∏è **N√£o usar em produ√ß√£o sem ajustes**:
- Single NAT = SPOF
- Sem HA para stateful apps
- Observabilidade b√°sica

### Para Produ√ß√£o

**Melhorias obrigat√≥rias antes de Go-Live:**

1. ‚úÖ Multi-AZ NAT Gateway
2. ‚úÖ Mix On-Demand + Spot (50/50)
3. ‚úÖ Backups autom√°ticos (Velero)
4. ‚úÖ Disaster Recovery testado
5. ‚úÖ Network Policies habilitadas
6. ‚úÖ Resource limits em todos pods
7. ‚úÖ Cluster Autoscaler configurado
8. ‚úÖ Alertas proativos completos
9. ‚úÖ Runbooks documentados
10. ‚úÖ On-call rotation definida

**Investimento adicional**: ~$300/m√™s AWS + 4 semanas eng time

### M√©tricas de Sucesso (6 meses)

- ‚úÖ 90%+ developers usando plataforma
- ‚úÖ Time-to-deploy: < 30 min (vs 5 dias antes)
- ‚úÖ Uptime: 99.5%+
- ‚úÖ Redu√ß√£o 80% tickets DevOps
- ‚úÖ NPS desenvolvedores: 50+
- ‚úÖ ROI: 300%+

---

**FIM DA AN√ÅLISE T√âCNICA**

**Pr√≥ximos passos**: Implementar Quick Wins e planejar roadmap de m√©dio prazo.
